---
title: "Changepoints for Environmental Data"
author: "Rebecca Killick(r.killick@lancs.ac.uk)"
date: "DSNE Autumn School 2022"
output:
  html_document
---

```{r,include=FALSE}
if(!require(patchwork)){
  install.packages('patchwork')
}
library(patchwork)
if(!require(changepoint)){
  install.packages('changepoint')
}
library(changepoint)
if(!require(EnvCpt)){
  install.packages('EnvCpt')
}
library(EnvCpt)
if(!require(ggplot2)){
  install.packages('ggplot2')
}
library(ggplot2)
knitr::opts_chunk$set(fig.align='center',fig.show='hold',fig.width=4,fig.height=4,size='footnotesize', cache=TRUE)
knitr::opts_knit$set(progress = FALSE,verbose = FALSE)
```

## Plan
* What are changepoints?
* How to find changepoints?
* How many changes?
* Important Considerations

This workshop is the practical side of what I presented this morning.

## Fitting changepoint models

```{r, echo=F, out.width='0.7\\textwidth'}
library(patchwork)
par(mar=c(4,4,.3,.3)) 
set.seed(1)
# Change in mean example following EFK
x=1:500
y=c(rnorm(100,1,sd=0.5),rnorm(150,0,sd=0.5),rnorm(200,2,sd=0.5),rnorm(50,0.5,sd=0.5))
fitted=c(rep(1,100),rep(0,150),rep(2,200),rep(0.5,50))
data=data.frame(x=x,y=y,fitted=fitted)
ggplot(data, aes(x=x, y=y)) + theme_minimal() +
  ylim(min(y), max(y)+0.5) +
  geom_point(colour="black",size=1) +
  geom_line(aes(x=x,y=fitted),colour="red",size=1) +
  annotate("text", x = 50, y = max(y)+0.3, label = "Model 1",colour="darkgreen", size=3) +
  annotate("text", x = 100, y = max(y)+0.3, label = "+",colour="red",size=5) +
  annotate("text", x = 175, y = max(y)+0.3, label = "Model 2",colour="darkgreen", size=3) +
  annotate("text", x = 250, y = max(y)+0.3, label = "+",colour="red",size=5) +
  annotate("text", x = 350, y = max(y)+0.3, label = "Model 3",colour="darkgreen", size=3) +
  annotate("text", x = 450, y = max(y)+0.3, label = "+",colour="red",size=5) +
  annotate("text", x = 490, y = max(y)+0.3, label = "Model 4",colour="darkgreen", size=3) 
```

## Packages
Today we will use the

`library(changepoint)`

and

`library(EnvCpt)`

packages.

Other notable `R` packages are available for changepoint analysis including

* `changepoint.np` - for nonparametric methods
* `strucchange` - for changes in regression
* `bcp` - if you want to be Bayesian
* `cpm` - for online changes (`changepoint.online` on github)
* `changepoint.influence` - for looking at influence diagnostics

## `changepoint` R package
The `changepoint` R package contains 3 wrapper functions:

* `cpt.mean` - mean only changes
* `cpt.var` - variance only changes
* `cpt.meanvar` - mean and variance changes

The package also contains:

* functions/methods for the `cpt` S4 class
* 5 data sets
* other R functions that are made available for those who know what they are doing and might want to extend/modify the package.


## The `cpt` class
* S4 class
* Slots store all the information from the analysis
    + e.g. `data.set`, `cpts`, `param.est`, `pen.value`, `ncpts.max`
* Slots are accessed via their names e.g. `cpts(x)`
* Standard methods are available for the class e.g. `plot`, `summary`
* Additional generic functions are available e.g. `seg.len`, `ncpts`
* Each core function outputs a `cpt` object

##  `cpt.mean`
`cpt.mean(data, penalty="MBIC", pen.value=0, method="AMOC", Q=5, test.stat="Normal", class=TRUE, param.estimates=TRUE,minseglen=1)`

* `data` - vector or `ts` object
* `penalty` - cut-off point, MBIC, SIC, BIC, AIC, Hannan-Quinn, Asymptotic, Manual. 
* `pen.value` - Type I error for Asymptotic, number or character for manual.
* `method` - AMOC, **PELT**, SegNeigh, BinSeg.
* `Q` - max number of changes for SegNeigh or BinSeg.
* `test.stat` - Test statistic, Normal or CUSUM.
* `class` - return a `cpt` object or not.
* `param.estimates` - return parameter estimates or not.
* `minseglen` - minimum number of data points between changes.


## Single Change in Mean
```{r, out.width='.3\\textwidth'}
set.seed(1)
m1=c(rnorm(100,0,1),rnorm(100,5,1))
m1.amoc=cpt.mean(m1)
cpts(m1.amoc)
```
## Single Change in Mean
```{r}
plot(m1.amoc)
```

## KEY POINT

* The `cpt.mean()` functions assumes a variance of 1.

* If your variance is larger/smaller then you will find fewer/more changes.

* Scale your data by `mad(data)` (Median Absolute Deviation) as a *more* robust method of variance estimation

## Task: Nile
Data from Cobb (1978): readings of the annual flow volume of the Nile River at Aswan from 1871 to 1970.
```{r,fig.height=3,fig.width=7,out.height='0.35\\textheight',out.width='\\textwidth'}
data(Nile)
ts.plot(Nile)
```

Hypothesized that there was a change around the turn of the century.  Use the  `cpt.mean` function to see if there is evidence for a change in mean in the Nile river data.




## Task: Nile
```{r}
# Put your code to analyse the Nile here
```


## Multiple Changes in `changepoint`
* At Most One Change (`AMOC`)

*Approximate* but computationally **fast**:

* Binary Segmenation (`BinSeg`) (Scott and Knott (1974)) which is $\mathcal{O}(n\log n)$ in CPU time.
	
*Slower* but **exact**:

* Segment Neighbourhood (`SegNeigh`) (Auger and Lawrence (1989)) is $\mathcal{O}(Qn^2)$.

**Fast** and **exact**:

* Pruned Exact Linear Time (`PELT`) (Killick et al. (2012)) At worst  $\mathcal{O}(n^2)$. For linear penalties <!-- $f(m)=m$ -->, scaling changes, $\mathcal{O}(n)$.


## cpt.var
`cpt.var(data, penalty, pen.value, know.mean=FALSE, mu=NA, method, Q, test.stat="Normal", class, param.estimates, minseglen=2)`

Majority of arguments are the same as for `cpt.mean`

* `know.mean` - if known we don't count it as an estimated parameter when calculating
penalties.
* `mu` - Mean if known.
* `test.stat` - Normal  or CSS (cumulative sums of squares)
* `minseglen` - Default is 2


## Changes in Variance
```{r,results='hold'}
set.seed(1)
v1=c(rnorm(100,0,1),rnorm(100,0,2),rnorm(100,0,10), 
     rnorm(100,0,9))
v1.man=cpt.var(v1,method='PELT',penalty='Manual',
     pen.value='2*log(n)')
cpts(v1.man)
param.est(v1.man)
```

## Changes in Variance
Ratios of true variances (4, 25, 0.81)
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(v1.man,cpt.width=3)
```

## `cpt.meanvar`
`cpt.meanvar(data, penalty, pen.value, method, Q, test.stat="Normal", class, param.estimates, shape=1,minseglen=2)`

Again the same underlying structure as `cpt.mean`.

* `test.stat` - choice of Normal, Gamma, Exponential, Poisson.
* `shape` - assumed shape parameter for Gamma.
* `minseglen` - minimum segment length of 2

## Mean & Variance
```{r}
set.seed(1)
mv1=c(rexp(50,rate=1),rexp(50,5),rexp(50,2),rexp(50,7))
mv1.pelt=cpt.meanvar(mv1,test.stat='Exponential',
      method='BinSeg',Q=10,penalty="SIC")
cpts(mv1.pelt)
param.est(mv1.pelt)
```

## Mean & Variance
```{r}
plot(mv1.pelt,cpt.width=3,cpt.col='blue')
```

## Task Air Quality
Use the air quality data, daily $O_3$ normalised measurements at several sites in London from 2020.  Use the "TH4" coded site.

Use the `cpt.meanvar` function to identify regions with different $O_3$ levels.

```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
load('AQ_data_KCL_2020.RData')
plot(df_2020_out[df_2020_out$code=="TH4",c(1,5)],type='l')
```

## A solution: TH4
```{r}
# Put your code for analysing site TH4 here
```


## Number of changes?
Does the number of changes appear reasonable?

## A solution: TH4
```{r}
# Have a play with penalty values and the number of changes they identify here.
```

## CROPS
**C**hangepoints for a **r**ange **o**f **p**enaltie**s**

Use `penalty='CROPS'` with `method='PELT'` to get all segmentations for a range of penalty values.

```{r}
v1.crops=cpt.var(v1,method="PELT",penalty="CROPS",
                 pen.value=c(5,500))
```
## CROPS
```{r}
cpts.full(v1.crops)
```
## CROPS
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
pen.value.full(v1.crops)
plot(v1.crops,ncpts=5)
```

## CROPS
```{r,fig.height=4,fig.width=4,out.height='0.7\\textheight'}
plot(v1.crops,diagnostic=TRUE)
```


## Task: Sea Ice
```{r,echo=FALSE}
seaice=c(5.61,10.09,7.54,9.68,5.72,4.83,5.25,6.09,6.29,6.18,5.2,5.56,6.07,4.09,4.72,2.98,3.27,5.27,0.82,9.43,5.35,4.49,4.37,4.13,4.65,4.97,5.17,4.54,5.49,2.65,2.23,2.09,1.65,1.75,2.12,1.28)
```
Yearly Sea Ice measurements for July-Sept for Barents from 1979 until 2014.

Use the `cpt.meanvar` function and CROPS to identify the number of changes.
```{r,fig.height=3,fig.width=7,out.height='0.4\\textheight',out.width='\\textwidth'}
plot(seq(from=1979,to=2014,by=1),seaice,type='l')
```


## A solution: Sea Ice
```{r}
# Put your code to analyse the Sea Ice data here
```


## EnvCpt
EnvCpt automatically fits 8 different models to your data:

* Flat mean (+AR1, +Change, +AR1+Change)
* Trend mean (+AR1, +Change, +AR1+Change)

AR1= autoregressive of order 1 = current data point is strongly related to the last data point.

**BONUS**: Can see which model is best

**PITFALL**: Might be best to use another model which isn't checked - always look at the fit!

## EnvCpt: Sea Ice
```{r,out.width='0.45\\textwidth'}
seaice.envcpt=envcpt(seaice,verbose=FALSE)
plot(seaice.envcpt)
plot(seaice.envcpt,type='aic')
```

## EnvCpt: Sea Ice
```{r,out.height='0.5\\textheight'}
cpts(seaice.envcpt[[9]])
plot(seaice.envcpt[[9]])
```

## Checking Assumptions
The main assumptions for a Normal likelihood ratio test for a change in mean are:

* Independent data points;
* Normal distributed points pre and post change;
* Constant variance across the data.

How can we check these?

## Checking Assumptions
In reality we can't check assumptions prior to analysis.
```{r,out.height='0.5\\textheight'}
ts.plot(m1)
```
## Checking Assumptions
```{r,out.height='0.6\\textheight'}
hist(m1)
```

## Checking Assumptions
```{r}
shapiro.test(m1)
ks.test(m1,pnorm,mean=mean(m1),sd=sd(m1))
```

## Checking Assumptions
```{r,out.height='0.6\\textheight'}
acf(m1)
```

## How to check
* Check the residuals
```{r}
means=param.est(m1.amoc)$mean
m1.resid=m1-rep(means,seg.len(m1.amoc))
shapiro.test(m1.resid)
```

## Residual Check
```{r}
ks.test(m1.resid,pnorm,mean=mean(m1.resid),sd=sd(m1.resid))
```

## Residual Check
```{r,out.height='0.6\\textheight'}
qqnorm(m1.resid)
qqline(m1.resid)
```

## Residual Check
```{r,out.height='0.6\\textheight'}
acf(m1.resid)
```

## Task
Check the assumptions you have made on the simulated, Nile, Sea Ice and $O_3$ data using either the segment or residual check. 

What effect might any invalid assumptions have on the inference?




## References - Stats
[JSS:](https://www.jstatsoft.org/article/view/v058i03) Killick, Eckley (2014)

[PELT:](http://www.tandfonline.com/doi/abs/10.1080/01621459.2012.737745) Killick, Fearnhead, Eckley (2012)  

[CROPS:](http://dx.doi.org/10.1080/10618600.2015.1116445) Kaynes, Eckley, Fearnhead (2015)  

## Reference - Env
[Walkthrough J. Climate:](https://journals.ametsoc.org/view/journals/clim/35/19/JCLI-D-21-0489.1.xml) Shi, Beaulieu, Killick, Lund (2022)

[Env. Pol.:](https://doi.org/10.1016/j.envpol.2022.118905) Tso, et al. (2022)

[Env. Model. & Soft:](https://doi.org/10.1016/j.envsoft.2021.104993) Hollaway, Henrys, Killick, Leeson, Watkins (2021)

[JASA:](https://asa.scitation.org/doi/10.1121/1.5126522) Hubert, Killick, Chung, Padovese (2019)

[EnvCpt J. Climate:](https://doi.org/10.1175/JCLI-D-17-0863.1) Beaulieu, and Killick (2018)

[J. Glac.:](https://doi.org/10.1017/jog.2018.44) Bunce, Carr, Nienow, Ross, Killick (2018)

[J. Glac.:](https://doi.org/10.1017/jog.2017.39) Leeson, A.A., et al. (2017)

[Cryo.:](https://doi.org/10.5194/tc-11-2149-2017) Carr, Bell, Killick, and Holt (2017)

[Hyd. Proc.:](http://onlinelibrary.wiley.com/doi/10.1002/hyp.9999/full) Wang, Killick, and Fu (2014)

[Ocean Eng.:](http://dx.doi.org/10.1016/j.oceaneng.2010.04.009) Killick, Eckley, Ewans, Jonathan (2010)
